{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text (replace with your preferred method)\n",
    "def preprocess_text(text):\n",
    "    # Lowercase, remove punctuation, tokenize (consider stemming/lemmatization)\n",
    "    text = text.lower()\n",
    "    text = ''.join([c for c in text if c.isalnum() or c.isspace()])\n",
    "    words = text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/nadun/Documents/projects/flower/simulation_env/data/stack_overflow/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(words)\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to numerical representation (one-hot encoding)\n",
    "def word_to_bow(word, vocab_size):\n",
    "    bow = torch.zeros(vocab_size)\n",
    "    if word in word_to_idx:\n",
    "        bow[word_to_idx[word]] = 1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert entire paragraph to BoW sequences\n",
    "bow_sequences = []\n",
    "for i in range(1, len(words)):\n",
    "    context = words[i-1]  # Use previous word as context\n",
    "    bow_sequences.append((word_to_bow(context, vocab_size), word_to_bow(words[i], vocab_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "all_inputs, all_targets = zip(*bow_sequences)\n",
    "inputs_tensor = torch.stack(all_inputs)\n",
    "targets_tensor = torch.stack(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple neural network (consider exploring more complex architectures for better results)\n",
    "class NextWordPredictor(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NextWordPredictor, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (adjust based on your dataset and task)\n",
    "input_size = vocab_size\n",
    "hidden_size = 128\n",
    "output_size = vocab_size  # Predict from same vocabulary\n",
    "\n",
    "model = NextWordPredictor(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer (consider experimenting with different options)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model (adjust epochs and batch size as needed)\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/6243], Loss: 2.2905\n",
      "Epoch [1/10], Step [101/6243], Loss: 2.7493\n",
      "Epoch [1/10], Step [201/6243], Loss: 2.1032\n",
      "Epoch [1/10], Step [301/6243], Loss: 2.8214\n",
      "Epoch [1/10], Step [401/6243], Loss: 2.3467\n",
      "Epoch [1/10], Step [501/6243], Loss: 2.3989\n",
      "Epoch [1/10], Step [601/6243], Loss: 2.8484\n",
      "Epoch [1/10], Step [701/6243], Loss: 2.4473\n",
      "Epoch [1/10], Step [801/6243], Loss: 2.9426\n",
      "Epoch [1/10], Step [901/6243], Loss: 2.8750\n",
      "Epoch [1/10], Step [1001/6243], Loss: 2.5477\n",
      "Epoch [1/10], Step [1101/6243], Loss: 3.0320\n",
      "Epoch [1/10], Step [1201/6243], Loss: 2.2162\n",
      "Epoch [1/10], Step [1301/6243], Loss: 2.6120\n",
      "Epoch [1/10], Step [1401/6243], Loss: 2.1599\n",
      "Epoch [1/10], Step [1501/6243], Loss: 2.2253\n",
      "Epoch [1/10], Step [1601/6243], Loss: 2.1433\n",
      "Epoch [1/10], Step [1701/6243], Loss: 2.6234\n",
      "Epoch [1/10], Step [1801/6243], Loss: 2.2519\n",
      "Epoch [1/10], Step [1901/6243], Loss: 2.4371\n",
      "Epoch [1/10], Step [2001/6243], Loss: 2.4474\n",
      "Epoch [1/10], Step [2101/6243], Loss: 2.2856\n",
      "Epoch [1/10], Step [2201/6243], Loss: 2.3181\n",
      "Epoch [1/10], Step [2301/6243], Loss: 2.8321\n",
      "Epoch [1/10], Step [2401/6243], Loss: 2.0583\n",
      "Epoch [1/10], Step [2501/6243], Loss: 2.6480\n",
      "Epoch [1/10], Step [2601/6243], Loss: 2.3925\n",
      "Epoch [1/10], Step [2701/6243], Loss: 2.2031\n",
      "Epoch [1/10], Step [2801/6243], Loss: 2.0600\n",
      "Epoch [1/10], Step [2901/6243], Loss: 2.4143\n",
      "Epoch [1/10], Step [3001/6243], Loss: 2.3154\n",
      "Epoch [1/10], Step [3101/6243], Loss: 2.7271\n",
      "Epoch [1/10], Step [3201/6243], Loss: 2.1313\n",
      "Epoch [1/10], Step [3301/6243], Loss: 2.3304\n",
      "Epoch [1/10], Step [3401/6243], Loss: 2.2363\n",
      "Epoch [1/10], Step [3501/6243], Loss: 2.5352\n",
      "Epoch [1/10], Step [3601/6243], Loss: 2.8548\n",
      "Epoch [1/10], Step [3701/6243], Loss: 2.5071\n",
      "Epoch [1/10], Step [3801/6243], Loss: 2.6827\n",
      "Epoch [1/10], Step [3901/6243], Loss: 2.2041\n",
      "Epoch [1/10], Step [4001/6243], Loss: 2.2628\n",
      "Epoch [1/10], Step [4101/6243], Loss: 2.5518\n",
      "Epoch [1/10], Step [4201/6243], Loss: 2.5368\n",
      "Epoch [1/10], Step [4301/6243], Loss: 2.2163\n",
      "Epoch [1/10], Step [4401/6243], Loss: 2.0653\n",
      "Epoch [1/10], Step [4501/6243], Loss: 2.6706\n",
      "Epoch [1/10], Step [4601/6243], Loss: 2.2280\n",
      "Epoch [1/10], Step [4701/6243], Loss: 2.8852\n",
      "Epoch [1/10], Step [4801/6243], Loss: 2.7742\n",
      "Epoch [1/10], Step [4901/6243], Loss: 2.6405\n",
      "Epoch [1/10], Step [5001/6243], Loss: 3.0642\n",
      "Epoch [1/10], Step [5101/6243], Loss: 1.8677\n",
      "Epoch [1/10], Step [5201/6243], Loss: 2.5234\n",
      "Epoch [1/10], Step [5301/6243], Loss: 2.3115\n",
      "Epoch [1/10], Step [5401/6243], Loss: 2.4677\n",
      "Epoch [1/10], Step [5501/6243], Loss: 2.4462\n",
      "Epoch [1/10], Step [5601/6243], Loss: 2.3416\n",
      "Epoch [1/10], Step [5701/6243], Loss: 2.2591\n",
      "Epoch [1/10], Step [5801/6243], Loss: 1.9890\n",
      "Epoch [1/10], Step [5901/6243], Loss: 2.3202\n",
      "Epoch [1/10], Step [6001/6243], Loss: 2.7401\n",
      "Epoch [1/10], Step [6101/6243], Loss: 2.0905\n",
      "Epoch [1/10], Step [6201/6243], Loss: 2.3189\n",
      "Epoch [2/10], Step [1/6243], Loss: 2.1700\n",
      "Epoch [2/10], Step [101/6243], Loss: 2.4329\n",
      "Epoch [2/10], Step [201/6243], Loss: 2.8863\n",
      "Epoch [2/10], Step [301/6243], Loss: 2.3364\n",
      "Epoch [2/10], Step [401/6243], Loss: 3.0003\n",
      "Epoch [2/10], Step [501/6243], Loss: 2.3579\n",
      "Epoch [2/10], Step [601/6243], Loss: 2.4725\n",
      "Epoch [2/10], Step [701/6243], Loss: 2.7520\n",
      "Epoch [2/10], Step [801/6243], Loss: 2.2136\n",
      "Epoch [2/10], Step [901/6243], Loss: 2.4568\n",
      "Epoch [2/10], Step [1001/6243], Loss: 2.7288\n",
      "Epoch [2/10], Step [1101/6243], Loss: 1.9265\n",
      "Epoch [2/10], Step [1201/6243], Loss: 2.5521\n",
      "Epoch [2/10], Step [1301/6243], Loss: 2.0988\n",
      "Epoch [2/10], Step [1401/6243], Loss: 2.5274\n",
      "Epoch [2/10], Step [1501/6243], Loss: 2.7121\n",
      "Epoch [2/10], Step [1601/6243], Loss: 1.9714\n",
      "Epoch [2/10], Step [1701/6243], Loss: 2.5685\n",
      "Epoch [2/10], Step [1801/6243], Loss: 2.5125\n",
      "Epoch [2/10], Step [1901/6243], Loss: 2.3526\n",
      "Epoch [2/10], Step [2001/6243], Loss: 2.2783\n",
      "Epoch [2/10], Step [2101/6243], Loss: 2.1490\n",
      "Epoch [2/10], Step [2201/6243], Loss: 2.5633\n",
      "Epoch [2/10], Step [2301/6243], Loss: 2.4574\n",
      "Epoch [2/10], Step [2401/6243], Loss: 2.5146\n",
      "Epoch [2/10], Step [2501/6243], Loss: 2.5282\n",
      "Epoch [2/10], Step [2601/6243], Loss: 2.4583\n",
      "Epoch [2/10], Step [2701/6243], Loss: 2.3627\n",
      "Epoch [2/10], Step [2801/6243], Loss: 2.9150\n",
      "Epoch [2/10], Step [2901/6243], Loss: 2.1660\n",
      "Epoch [2/10], Step [3001/6243], Loss: 2.3055\n",
      "Epoch [2/10], Step [3101/6243], Loss: 2.6185\n",
      "Epoch [2/10], Step [3201/6243], Loss: 2.5035\n",
      "Epoch [2/10], Step [3301/6243], Loss: 2.2067\n",
      "Epoch [2/10], Step [3401/6243], Loss: 2.1361\n",
      "Epoch [2/10], Step [3501/6243], Loss: 2.5614\n",
      "Epoch [2/10], Step [3601/6243], Loss: 2.3682\n",
      "Epoch [2/10], Step [3701/6243], Loss: 2.3471\n",
      "Epoch [2/10], Step [3801/6243], Loss: 2.6203\n",
      "Epoch [2/10], Step [3901/6243], Loss: 2.4781\n",
      "Epoch [2/10], Step [4001/6243], Loss: 2.5564\n",
      "Epoch [2/10], Step [4101/6243], Loss: 2.1367\n",
      "Epoch [2/10], Step [4201/6243], Loss: 2.2047\n",
      "Epoch [2/10], Step [4301/6243], Loss: 2.5849\n",
      "Epoch [2/10], Step [4401/6243], Loss: 2.3325\n",
      "Epoch [2/10], Step [4501/6243], Loss: 2.3644\n",
      "Epoch [2/10], Step [4601/6243], Loss: 2.6711\n",
      "Epoch [2/10], Step [4701/6243], Loss: 2.3884\n",
      "Epoch [2/10], Step [4801/6243], Loss: 2.2999\n",
      "Epoch [2/10], Step [4901/6243], Loss: 2.3918\n",
      "Epoch [2/10], Step [5001/6243], Loss: 2.3914\n",
      "Epoch [2/10], Step [5101/6243], Loss: 2.6325\n",
      "Epoch [2/10], Step [5201/6243], Loss: 2.1970\n",
      "Epoch [2/10], Step [5301/6243], Loss: 2.1948\n",
      "Epoch [2/10], Step [5401/6243], Loss: 2.2228\n",
      "Epoch [2/10], Step [5501/6243], Loss: 2.3765\n",
      "Epoch [2/10], Step [5601/6243], Loss: 2.2330\n",
      "Epoch [2/10], Step [5701/6243], Loss: 2.5687\n",
      "Epoch [2/10], Step [5801/6243], Loss: 2.6015\n",
      "Epoch [2/10], Step [5901/6243], Loss: 2.5222\n",
      "Epoch [2/10], Step [6001/6243], Loss: 2.4457\n",
      "Epoch [2/10], Step [6101/6243], Loss: 2.3979\n",
      "Epoch [2/10], Step [6201/6243], Loss: 2.8572\n",
      "Epoch [3/10], Step [1/6243], Loss: 2.4325\n",
      "Epoch [3/10], Step [101/6243], Loss: 2.1000\n",
      "Epoch [3/10], Step [201/6243], Loss: 2.7252\n",
      "Epoch [3/10], Step [301/6243], Loss: 2.1258\n",
      "Epoch [3/10], Step [401/6243], Loss: 2.3337\n",
      "Epoch [3/10], Step [501/6243], Loss: 2.2607\n",
      "Epoch [3/10], Step [601/6243], Loss: 2.6591\n",
      "Epoch [3/10], Step [701/6243], Loss: 2.2462\n",
      "Epoch [3/10], Step [801/6243], Loss: 2.5114\n",
      "Epoch [3/10], Step [901/6243], Loss: 2.5532\n",
      "Epoch [3/10], Step [1001/6243], Loss: 1.7045\n",
      "Epoch [3/10], Step [1101/6243], Loss: 2.2265\n",
      "Epoch [3/10], Step [1201/6243], Loss: 2.3949\n",
      "Epoch [3/10], Step [1301/6243], Loss: 2.1818\n",
      "Epoch [3/10], Step [1401/6243], Loss: 2.3079\n",
      "Epoch [3/10], Step [1501/6243], Loss: 2.4827\n",
      "Epoch [3/10], Step [1601/6243], Loss: 2.6610\n",
      "Epoch [3/10], Step [1701/6243], Loss: 2.3860\n",
      "Epoch [3/10], Step [1801/6243], Loss: 2.4029\n",
      "Epoch [3/10], Step [1901/6243], Loss: 2.7847\n",
      "Epoch [3/10], Step [2001/6243], Loss: 1.9481\n",
      "Epoch [3/10], Step [2101/6243], Loss: 2.7092\n",
      "Epoch [3/10], Step [2201/6243], Loss: 2.8111\n",
      "Epoch [3/10], Step [2301/6243], Loss: 2.5206\n",
      "Epoch [3/10], Step [2401/6243], Loss: 2.2863\n",
      "Epoch [3/10], Step [2501/6243], Loss: 2.1395\n",
      "Epoch [3/10], Step [2601/6243], Loss: 2.4943\n",
      "Epoch [3/10], Step [2701/6243], Loss: 2.7657\n",
      "Epoch [3/10], Step [2801/6243], Loss: 2.4585\n",
      "Epoch [3/10], Step [2901/6243], Loss: 2.6883\n",
      "Epoch [3/10], Step [3001/6243], Loss: 2.7752\n",
      "Epoch [3/10], Step [3101/6243], Loss: 2.6293\n",
      "Epoch [3/10], Step [3201/6243], Loss: 2.0118\n",
      "Epoch [3/10], Step [3301/6243], Loss: 2.3243\n",
      "Epoch [3/10], Step [3401/6243], Loss: 2.5856\n",
      "Epoch [3/10], Step [3501/6243], Loss: 2.4707\n",
      "Epoch [3/10], Step [3601/6243], Loss: 2.2369\n",
      "Epoch [3/10], Step [3701/6243], Loss: 2.0962\n",
      "Epoch [3/10], Step [3801/6243], Loss: 2.6293\n",
      "Epoch [3/10], Step [3901/6243], Loss: 2.4340\n",
      "Epoch [3/10], Step [4001/6243], Loss: 2.5161\n",
      "Epoch [3/10], Step [4101/6243], Loss: 2.3388\n",
      "Epoch [3/10], Step [4201/6243], Loss: 2.4092\n",
      "Epoch [3/10], Step [4301/6243], Loss: 2.6373\n",
      "Epoch [3/10], Step [4401/6243], Loss: 2.4181\n",
      "Epoch [3/10], Step [4501/6243], Loss: 2.1087\n",
      "Epoch [3/10], Step [4601/6243], Loss: 2.6814\n",
      "Epoch [3/10], Step [4701/6243], Loss: 2.3428\n",
      "Epoch [3/10], Step [4801/6243], Loss: 2.2950\n",
      "Epoch [3/10], Step [4901/6243], Loss: 3.1378\n",
      "Epoch [3/10], Step [5001/6243], Loss: 2.3070\n",
      "Epoch [3/10], Step [5101/6243], Loss: 2.4208\n",
      "Epoch [3/10], Step [5201/6243], Loss: 2.2201\n",
      "Epoch [3/10], Step [5301/6243], Loss: 2.4638\n",
      "Epoch [3/10], Step [5401/6243], Loss: 2.3334\n",
      "Epoch [3/10], Step [5501/6243], Loss: 2.3402\n",
      "Epoch [3/10], Step [5601/6243], Loss: 2.9497\n",
      "Epoch [3/10], Step [5701/6243], Loss: 1.8676\n",
      "Epoch [3/10], Step [5801/6243], Loss: 2.1250\n",
      "Epoch [3/10], Step [5901/6243], Loss: 2.7651\n",
      "Epoch [3/10], Step [6001/6243], Loss: 2.6052\n",
      "Epoch [3/10], Step [6101/6243], Loss: 2.4552\n",
      "Epoch [3/10], Step [6201/6243], Loss: 2.3790\n",
      "Epoch [4/10], Step [1/6243], Loss: 2.5376\n",
      "Epoch [4/10], Step [101/6243], Loss: 2.3994\n",
      "Epoch [4/10], Step [201/6243], Loss: 2.2310\n",
      "Epoch [4/10], Step [301/6243], Loss: 2.4018\n",
      "Epoch [4/10], Step [401/6243], Loss: 2.5872\n",
      "Epoch [4/10], Step [501/6243], Loss: 2.7054\n",
      "Epoch [4/10], Step [601/6243], Loss: 2.4102\n",
      "Epoch [4/10], Step [701/6243], Loss: 2.4392\n",
      "Epoch [4/10], Step [801/6243], Loss: 2.3093\n",
      "Epoch [4/10], Step [901/6243], Loss: 2.5343\n",
      "Epoch [4/10], Step [1001/6243], Loss: 2.1604\n",
      "Epoch [4/10], Step [1101/6243], Loss: 2.4406\n",
      "Epoch [4/10], Step [1201/6243], Loss: 2.1203\n",
      "Epoch [4/10], Step [1301/6243], Loss: 2.4081\n",
      "Epoch [4/10], Step [1401/6243], Loss: 2.0071\n",
      "Epoch [4/10], Step [1501/6243], Loss: 2.0224\n",
      "Epoch [4/10], Step [1601/6243], Loss: 2.6458\n",
      "Epoch [4/10], Step [1701/6243], Loss: 2.4438\n",
      "Epoch [4/10], Step [1801/6243], Loss: 2.2181\n",
      "Epoch [4/10], Step [1901/6243], Loss: 2.3464\n",
      "Epoch [4/10], Step [2001/6243], Loss: 2.6841\n",
      "Epoch [4/10], Step [2101/6243], Loss: 2.7493\n",
      "Epoch [4/10], Step [2201/6243], Loss: 2.5293\n",
      "Epoch [4/10], Step [2301/6243], Loss: 2.0984\n",
      "Epoch [4/10], Step [2401/6243], Loss: 2.2893\n",
      "Epoch [4/10], Step [2501/6243], Loss: 2.4435\n",
      "Epoch [4/10], Step [2601/6243], Loss: 2.4466\n",
      "Epoch [4/10], Step [2701/6243], Loss: 2.4998\n",
      "Epoch [4/10], Step [2801/6243], Loss: 2.1877\n",
      "Epoch [4/10], Step [2901/6243], Loss: 2.8522\n",
      "Epoch [4/10], Step [3001/6243], Loss: 2.2907\n",
      "Epoch [4/10], Step [3101/6243], Loss: 2.3083\n",
      "Epoch [4/10], Step [3201/6243], Loss: 2.7628\n",
      "Epoch [4/10], Step [3301/6243], Loss: 2.2679\n",
      "Epoch [4/10], Step [3401/6243], Loss: 1.9940\n",
      "Epoch [4/10], Step [3501/6243], Loss: 2.0222\n",
      "Epoch [4/10], Step [3601/6243], Loss: 2.4062\n",
      "Epoch [4/10], Step [3701/6243], Loss: 2.3540\n",
      "Epoch [4/10], Step [3801/6243], Loss: 2.1508\n",
      "Epoch [4/10], Step [3901/6243], Loss: 2.6881\n",
      "Epoch [4/10], Step [4001/6243], Loss: 2.0890\n",
      "Epoch [4/10], Step [4101/6243], Loss: 2.4198\n",
      "Epoch [4/10], Step [4201/6243], Loss: 3.0460\n",
      "Epoch [4/10], Step [4301/6243], Loss: 2.3261\n",
      "Epoch [4/10], Step [4401/6243], Loss: 2.3578\n",
      "Epoch [4/10], Step [4501/6243], Loss: 2.3593\n",
      "Epoch [4/10], Step [4601/6243], Loss: 2.4739\n",
      "Epoch [4/10], Step [4701/6243], Loss: 2.8953\n",
      "Epoch [4/10], Step [4801/6243], Loss: 2.2401\n",
      "Epoch [4/10], Step [4901/6243], Loss: 2.0448\n",
      "Epoch [4/10], Step [5001/6243], Loss: 2.5903\n",
      "Epoch [4/10], Step [5101/6243], Loss: 2.1617\n",
      "Epoch [4/10], Step [5201/6243], Loss: 2.5488\n",
      "Epoch [4/10], Step [5301/6243], Loss: 2.5967\n",
      "Epoch [4/10], Step [5401/6243], Loss: 2.8897\n",
      "Epoch [4/10], Step [5501/6243], Loss: 2.4398\n",
      "Epoch [4/10], Step [5601/6243], Loss: 2.4810\n",
      "Epoch [4/10], Step [5701/6243], Loss: 2.3885\n",
      "Epoch [4/10], Step [5801/6243], Loss: 2.4819\n",
      "Epoch [4/10], Step [5901/6243], Loss: 2.2926\n",
      "Epoch [4/10], Step [6001/6243], Loss: 1.9642\n",
      "Epoch [4/10], Step [6101/6243], Loss: 3.0573\n",
      "Epoch [4/10], Step [6201/6243], Loss: 2.1383\n",
      "Epoch [5/10], Step [1/6243], Loss: 2.0539\n",
      "Epoch [5/10], Step [101/6243], Loss: 2.5164\n",
      "Epoch [5/10], Step [201/6243], Loss: 2.2990\n",
      "Epoch [5/10], Step [301/6243], Loss: 2.5033\n",
      "Epoch [5/10], Step [401/6243], Loss: 2.3158\n",
      "Epoch [5/10], Step [501/6243], Loss: 2.7359\n",
      "Epoch [5/10], Step [601/6243], Loss: 2.7401\n",
      "Epoch [5/10], Step [701/6243], Loss: 2.6016\n",
      "Epoch [5/10], Step [801/6243], Loss: 2.9950\n",
      "Epoch [5/10], Step [901/6243], Loss: 2.5522\n",
      "Epoch [5/10], Step [1001/6243], Loss: 2.5570\n",
      "Epoch [5/10], Step [1101/6243], Loss: 2.4772\n",
      "Epoch [5/10], Step [1201/6243], Loss: 2.2535\n",
      "Epoch [5/10], Step [1301/6243], Loss: 2.0487\n",
      "Epoch [5/10], Step [1401/6243], Loss: 2.3285\n",
      "Epoch [5/10], Step [1501/6243], Loss: 2.2714\n",
      "Epoch [5/10], Step [1601/6243], Loss: 2.4771\n",
      "Epoch [5/10], Step [1701/6243], Loss: 2.2689\n",
      "Epoch [5/10], Step [1801/6243], Loss: 3.1756\n",
      "Epoch [5/10], Step [1901/6243], Loss: 2.2245\n",
      "Epoch [5/10], Step [2001/6243], Loss: 2.7381\n",
      "Epoch [5/10], Step [2101/6243], Loss: 1.7799\n",
      "Epoch [5/10], Step [2201/6243], Loss: 2.5781\n",
      "Epoch [5/10], Step [2301/6243], Loss: 2.0394\n",
      "Epoch [5/10], Step [2401/6243], Loss: 2.4890\n",
      "Epoch [5/10], Step [2501/6243], Loss: 2.4291\n",
      "Epoch [5/10], Step [2601/6243], Loss: 2.5941\n",
      "Epoch [5/10], Step [2701/6243], Loss: 2.1229\n",
      "Epoch [5/10], Step [2801/6243], Loss: 2.7532\n",
      "Epoch [5/10], Step [2901/6243], Loss: 2.7952\n",
      "Epoch [5/10], Step [3001/6243], Loss: 2.5433\n",
      "Epoch [5/10], Step [3101/6243], Loss: 2.7424\n",
      "Epoch [5/10], Step [3201/6243], Loss: 2.1881\n",
      "Epoch [5/10], Step [3301/6243], Loss: 2.0601\n",
      "Epoch [5/10], Step [3401/6243], Loss: 2.5423\n",
      "Epoch [5/10], Step [3501/6243], Loss: 2.7166\n",
      "Epoch [5/10], Step [3601/6243], Loss: 2.4974\n",
      "Epoch [5/10], Step [3701/6243], Loss: 2.4693\n",
      "Epoch [5/10], Step [3801/6243], Loss: 2.6814\n",
      "Epoch [5/10], Step [3901/6243], Loss: 2.3854\n",
      "Epoch [5/10], Step [4001/6243], Loss: 2.1615\n",
      "Epoch [5/10], Step [4101/6243], Loss: 2.5229\n",
      "Epoch [5/10], Step [4201/6243], Loss: 2.5483\n",
      "Epoch [5/10], Step [4301/6243], Loss: 3.0435\n",
      "Epoch [5/10], Step [4401/6243], Loss: 2.4911\n",
      "Epoch [5/10], Step [4501/6243], Loss: 2.9469\n",
      "Epoch [5/10], Step [4601/6243], Loss: 2.1983\n",
      "Epoch [5/10], Step [4701/6243], Loss: 2.7253\n",
      "Epoch [5/10], Step [4801/6243], Loss: 2.9697\n",
      "Epoch [5/10], Step [4901/6243], Loss: 2.6762\n",
      "Epoch [5/10], Step [5001/6243], Loss: 3.2561\n",
      "Epoch [5/10], Step [5101/6243], Loss: 2.2486\n",
      "Epoch [5/10], Step [5201/6243], Loss: 2.0849\n",
      "Epoch [5/10], Step [5301/6243], Loss: 2.2628\n",
      "Epoch [5/10], Step [5401/6243], Loss: 2.4122\n",
      "Epoch [5/10], Step [5501/6243], Loss: 2.3862\n",
      "Epoch [5/10], Step [5601/6243], Loss: 2.2767\n",
      "Epoch [5/10], Step [5701/6243], Loss: 2.4360\n",
      "Epoch [5/10], Step [5801/6243], Loss: 2.0937\n",
      "Epoch [5/10], Step [5901/6243], Loss: 2.4719\n",
      "Epoch [5/10], Step [6001/6243], Loss: 2.2798\n",
      "Epoch [5/10], Step [6101/6243], Loss: 2.4283\n",
      "Epoch [5/10], Step [6201/6243], Loss: 2.4510\n",
      "Epoch [6/10], Step [1/6243], Loss: 2.4143\n",
      "Epoch [6/10], Step [101/6243], Loss: 2.0346\n",
      "Epoch [6/10], Step [201/6243], Loss: 2.3241\n",
      "Epoch [6/10], Step [301/6243], Loss: 2.5974\n",
      "Epoch [6/10], Step [401/6243], Loss: 2.3659\n",
      "Epoch [6/10], Step [501/6243], Loss: 2.3487\n",
      "Epoch [6/10], Step [601/6243], Loss: 2.3055\n",
      "Epoch [6/10], Step [701/6243], Loss: 2.2392\n",
      "Epoch [6/10], Step [801/6243], Loss: 2.4138\n",
      "Epoch [6/10], Step [901/6243], Loss: 2.4640\n",
      "Epoch [6/10], Step [1001/6243], Loss: 2.6207\n",
      "Epoch [6/10], Step [1101/6243], Loss: 2.6098\n",
      "Epoch [6/10], Step [1201/6243], Loss: 2.3994\n",
      "Epoch [6/10], Step [1301/6243], Loss: 2.2530\n",
      "Epoch [6/10], Step [1401/6243], Loss: 3.0610\n",
      "Epoch [6/10], Step [1501/6243], Loss: 2.1142\n",
      "Epoch [6/10], Step [1601/6243], Loss: 2.4353\n",
      "Epoch [6/10], Step [1701/6243], Loss: 2.2977\n",
      "Epoch [6/10], Step [1801/6243], Loss: 2.6425\n",
      "Epoch [6/10], Step [1901/6243], Loss: 2.3038\n",
      "Epoch [6/10], Step [2001/6243], Loss: 2.8555\n",
      "Epoch [6/10], Step [2101/6243], Loss: 2.6412\n",
      "Epoch [6/10], Step [2201/6243], Loss: 2.3678\n",
      "Epoch [6/10], Step [2301/6243], Loss: 2.5116\n",
      "Epoch [6/10], Step [2401/6243], Loss: 2.5207\n",
      "Epoch [6/10], Step [2501/6243], Loss: 2.2895\n",
      "Epoch [6/10], Step [2601/6243], Loss: 2.1524\n",
      "Epoch [6/10], Step [2701/6243], Loss: 2.5550\n",
      "Epoch [6/10], Step [2801/6243], Loss: 2.6606\n",
      "Epoch [6/10], Step [2901/6243], Loss: 2.3813\n",
      "Epoch [6/10], Step [3001/6243], Loss: 2.2122\n",
      "Epoch [6/10], Step [3101/6243], Loss: 1.8970\n",
      "Epoch [6/10], Step [3201/6243], Loss: 2.4031\n",
      "Epoch [6/10], Step [3301/6243], Loss: 2.7735\n",
      "Epoch [6/10], Step [3401/6243], Loss: 2.8387\n",
      "Epoch [6/10], Step [3501/6243], Loss: 2.7132\n",
      "Epoch [6/10], Step [3601/6243], Loss: 2.4863\n",
      "Epoch [6/10], Step [3701/6243], Loss: 1.9590\n",
      "Epoch [6/10], Step [3801/6243], Loss: 2.1064\n",
      "Epoch [6/10], Step [3901/6243], Loss: 2.1219\n",
      "Epoch [6/10], Step [4001/6243], Loss: 2.3139\n",
      "Epoch [6/10], Step [4101/6243], Loss: 2.1236\n",
      "Epoch [6/10], Step [4201/6243], Loss: 2.4899\n",
      "Epoch [6/10], Step [4301/6243], Loss: 1.7764\n",
      "Epoch [6/10], Step [4401/6243], Loss: 2.7028\n",
      "Epoch [6/10], Step [4501/6243], Loss: 2.4070\n",
      "Epoch [6/10], Step [4601/6243], Loss: 2.1426\n",
      "Epoch [6/10], Step [4701/6243], Loss: 2.1785\n",
      "Epoch [6/10], Step [4801/6243], Loss: 2.4992\n",
      "Epoch [6/10], Step [4901/6243], Loss: 2.3182\n",
      "Epoch [6/10], Step [5001/6243], Loss: 2.2785\n",
      "Epoch [6/10], Step [5101/6243], Loss: 2.4815\n",
      "Epoch [6/10], Step [5201/6243], Loss: 2.7238\n",
      "Epoch [6/10], Step [5301/6243], Loss: 2.4565\n",
      "Epoch [6/10], Step [5401/6243], Loss: 2.3744\n",
      "Epoch [6/10], Step [5501/6243], Loss: 2.7625\n",
      "Epoch [6/10], Step [5601/6243], Loss: 2.8216\n",
      "Epoch [6/10], Step [5701/6243], Loss: 2.2967\n",
      "Epoch [6/10], Step [5801/6243], Loss: 2.1323\n",
      "Epoch [6/10], Step [5901/6243], Loss: 2.6071\n",
      "Epoch [6/10], Step [6001/6243], Loss: 2.2224\n",
      "Epoch [6/10], Step [6101/6243], Loss: 2.5062\n",
      "Epoch [6/10], Step [6201/6243], Loss: 2.2052\n",
      "Epoch [7/10], Step [1/6243], Loss: 2.7397\n",
      "Epoch [7/10], Step [101/6243], Loss: 2.2896\n",
      "Epoch [7/10], Step [201/6243], Loss: 2.6269\n",
      "Epoch [7/10], Step [301/6243], Loss: 2.5886\n",
      "Epoch [7/10], Step [401/6243], Loss: 2.8328\n",
      "Epoch [7/10], Step [501/6243], Loss: 2.9450\n",
      "Epoch [7/10], Step [601/6243], Loss: 2.3725\n",
      "Epoch [7/10], Step [701/6243], Loss: 2.4058\n",
      "Epoch [7/10], Step [801/6243], Loss: 2.6141\n",
      "Epoch [7/10], Step [901/6243], Loss: 2.3481\n",
      "Epoch [7/10], Step [1001/6243], Loss: 2.1280\n",
      "Epoch [7/10], Step [1101/6243], Loss: 2.3569\n",
      "Epoch [7/10], Step [1201/6243], Loss: 2.4928\n",
      "Epoch [7/10], Step [1301/6243], Loss: 2.2056\n",
      "Epoch [7/10], Step [1401/6243], Loss: 2.8481\n",
      "Epoch [7/10], Step [1501/6243], Loss: 2.5627\n",
      "Epoch [7/10], Step [1601/6243], Loss: 2.4398\n",
      "Epoch [7/10], Step [1701/6243], Loss: 2.8400\n",
      "Epoch [7/10], Step [1801/6243], Loss: 2.3775\n",
      "Epoch [7/10], Step [1901/6243], Loss: 2.4465\n",
      "Epoch [7/10], Step [2001/6243], Loss: 2.5671\n",
      "Epoch [7/10], Step [2101/6243], Loss: 2.5333\n",
      "Epoch [7/10], Step [2201/6243], Loss: 2.0297\n",
      "Epoch [7/10], Step [2301/6243], Loss: 2.3264\n",
      "Epoch [7/10], Step [2401/6243], Loss: 2.3957\n",
      "Epoch [7/10], Step [2501/6243], Loss: 2.1595\n",
      "Epoch [7/10], Step [2601/6243], Loss: 2.9777\n",
      "Epoch [7/10], Step [2701/6243], Loss: 2.6754\n",
      "Epoch [7/10], Step [2801/6243], Loss: 2.1792\n",
      "Epoch [7/10], Step [2901/6243], Loss: 2.0863\n",
      "Epoch [7/10], Step [3001/6243], Loss: 2.6557\n",
      "Epoch [7/10], Step [3101/6243], Loss: 2.5986\n",
      "Epoch [7/10], Step [3201/6243], Loss: 2.5855\n",
      "Epoch [7/10], Step [3301/6243], Loss: 2.2883\n",
      "Epoch [7/10], Step [3401/6243], Loss: 2.3009\n",
      "Epoch [7/10], Step [3501/6243], Loss: 2.5389\n",
      "Epoch [7/10], Step [3601/6243], Loss: 2.3404\n",
      "Epoch [7/10], Step [3701/6243], Loss: 2.2346\n",
      "Epoch [7/10], Step [3801/6243], Loss: 3.0270\n",
      "Epoch [7/10], Step [3901/6243], Loss: 2.5186\n",
      "Epoch [7/10], Step [4001/6243], Loss: 2.0770\n",
      "Epoch [7/10], Step [4101/6243], Loss: 2.5029\n",
      "Epoch [7/10], Step [4201/6243], Loss: 2.8513\n",
      "Epoch [7/10], Step [4301/6243], Loss: 2.3924\n",
      "Epoch [7/10], Step [4401/6243], Loss: 2.4772\n",
      "Epoch [7/10], Step [4501/6243], Loss: 2.5929\n",
      "Epoch [7/10], Step [4601/6243], Loss: 2.0769\n",
      "Epoch [7/10], Step [4701/6243], Loss: 2.7174\n",
      "Epoch [7/10], Step [4801/6243], Loss: 2.5198\n",
      "Epoch [7/10], Step [4901/6243], Loss: 2.3722\n",
      "Epoch [7/10], Step [5001/6243], Loss: 2.1630\n",
      "Epoch [7/10], Step [5101/6243], Loss: 2.3895\n",
      "Epoch [7/10], Step [5201/6243], Loss: 2.3954\n",
      "Epoch [7/10], Step [5301/6243], Loss: 2.3012\n",
      "Epoch [7/10], Step [5401/6243], Loss: 2.7424\n",
      "Epoch [7/10], Step [5501/6243], Loss: 2.5536\n",
      "Epoch [7/10], Step [5601/6243], Loss: 2.0809\n",
      "Epoch [7/10], Step [5701/6243], Loss: 2.6805\n",
      "Epoch [7/10], Step [5801/6243], Loss: 2.1931\n",
      "Epoch [7/10], Step [5901/6243], Loss: 2.4586\n",
      "Epoch [7/10], Step [6001/6243], Loss: 2.5053\n",
      "Epoch [7/10], Step [6101/6243], Loss: 2.8337\n",
      "Epoch [7/10], Step [6201/6243], Loss: 2.3576\n",
      "Epoch [8/10], Step [1/6243], Loss: 2.1642\n",
      "Epoch [8/10], Step [101/6243], Loss: 2.7372\n",
      "Epoch [8/10], Step [201/6243], Loss: 2.1078\n",
      "Epoch [8/10], Step [301/6243], Loss: 2.1721\n",
      "Epoch [8/10], Step [401/6243], Loss: 2.7665\n",
      "Epoch [8/10], Step [501/6243], Loss: 2.1365\n",
      "Epoch [8/10], Step [601/6243], Loss: 2.3113\n",
      "Epoch [8/10], Step [701/6243], Loss: 1.9085\n",
      "Epoch [8/10], Step [801/6243], Loss: 2.4790\n",
      "Epoch [8/10], Step [901/6243], Loss: 2.1411\n",
      "Epoch [8/10], Step [1001/6243], Loss: 2.2777\n",
      "Epoch [8/10], Step [1101/6243], Loss: 2.6585\n",
      "Epoch [8/10], Step [1201/6243], Loss: 2.5278\n",
      "Epoch [8/10], Step [1301/6243], Loss: 2.3914\n",
      "Epoch [8/10], Step [1401/6243], Loss: 2.4026\n",
      "Epoch [8/10], Step [1501/6243], Loss: 2.2212\n",
      "Epoch [8/10], Step [1601/6243], Loss: 2.4344\n",
      "Epoch [8/10], Step [1701/6243], Loss: 2.5840\n",
      "Epoch [8/10], Step [1801/6243], Loss: 2.3318\n",
      "Epoch [8/10], Step [1901/6243], Loss: 2.5738\n",
      "Epoch [8/10], Step [2001/6243], Loss: 2.6628\n",
      "Epoch [8/10], Step [2101/6243], Loss: 2.6190\n",
      "Epoch [8/10], Step [2201/6243], Loss: 1.9886\n",
      "Epoch [8/10], Step [2301/6243], Loss: 2.4695\n",
      "Epoch [8/10], Step [2401/6243], Loss: 2.6003\n",
      "Epoch [8/10], Step [2501/6243], Loss: 2.5453\n",
      "Epoch [8/10], Step [2601/6243], Loss: 2.4208\n",
      "Epoch [8/10], Step [2701/6243], Loss: 2.4131\n",
      "Epoch [8/10], Step [2801/6243], Loss: 2.8098\n",
      "Epoch [8/10], Step [2901/6243], Loss: 2.8153\n",
      "Epoch [8/10], Step [3001/6243], Loss: 2.5566\n",
      "Epoch [8/10], Step [3101/6243], Loss: 2.6376\n",
      "Epoch [8/10], Step [3201/6243], Loss: 2.1175\n",
      "Epoch [8/10], Step [3301/6243], Loss: 2.3527\n",
      "Epoch [8/10], Step [3401/6243], Loss: 2.3258\n",
      "Epoch [8/10], Step [3501/6243], Loss: 2.4839\n",
      "Epoch [8/10], Step [3601/6243], Loss: 2.2000\n",
      "Epoch [8/10], Step [3701/6243], Loss: 2.7538\n",
      "Epoch [8/10], Step [3801/6243], Loss: 2.4846\n",
      "Epoch [8/10], Step [3901/6243], Loss: 2.2981\n",
      "Epoch [8/10], Step [4001/6243], Loss: 2.6787\n",
      "Epoch [8/10], Step [4101/6243], Loss: 2.4215\n",
      "Epoch [8/10], Step [4201/6243], Loss: 2.5215\n",
      "Epoch [8/10], Step [4301/6243], Loss: 2.4002\n",
      "Epoch [8/10], Step [4401/6243], Loss: 2.1654\n",
      "Epoch [8/10], Step [4501/6243], Loss: 2.9726\n",
      "Epoch [8/10], Step [4601/6243], Loss: 3.0363\n",
      "Epoch [8/10], Step [4701/6243], Loss: 2.4530\n",
      "Epoch [8/10], Step [4801/6243], Loss: 2.6081\n",
      "Epoch [8/10], Step [4901/6243], Loss: 2.6266\n",
      "Epoch [8/10], Step [5001/6243], Loss: 2.2839\n",
      "Epoch [8/10], Step [5101/6243], Loss: 2.5028\n",
      "Epoch [8/10], Step [5201/6243], Loss: 2.3105\n",
      "Epoch [8/10], Step [5301/6243], Loss: 2.1287\n",
      "Epoch [8/10], Step [5401/6243], Loss: 2.6454\n",
      "Epoch [8/10], Step [5501/6243], Loss: 2.5105\n",
      "Epoch [8/10], Step [5601/6243], Loss: 2.4278\n",
      "Epoch [8/10], Step [5701/6243], Loss: 2.7107\n",
      "Epoch [8/10], Step [5801/6243], Loss: 2.6105\n",
      "Epoch [8/10], Step [5901/6243], Loss: 2.2502\n",
      "Epoch [8/10], Step [6001/6243], Loss: 2.3011\n",
      "Epoch [8/10], Step [6101/6243], Loss: 2.4605\n",
      "Epoch [8/10], Step [6201/6243], Loss: 2.2636\n",
      "Epoch [9/10], Step [1/6243], Loss: 2.3337\n",
      "Epoch [9/10], Step [101/6243], Loss: 2.6670\n",
      "Epoch [9/10], Step [201/6243], Loss: 1.9607\n",
      "Epoch [9/10], Step [301/6243], Loss: 2.6395\n",
      "Epoch [9/10], Step [401/6243], Loss: 2.4843\n",
      "Epoch [9/10], Step [501/6243], Loss: 2.3188\n",
      "Epoch [9/10], Step [601/6243], Loss: 2.5650\n",
      "Epoch [9/10], Step [701/6243], Loss: 2.4452\n",
      "Epoch [9/10], Step [801/6243], Loss: 2.2500\n",
      "Epoch [9/10], Step [901/6243], Loss: 2.1190\n",
      "Epoch [9/10], Step [1001/6243], Loss: 2.5965\n",
      "Epoch [9/10], Step [1101/6243], Loss: 2.1265\n",
      "Epoch [9/10], Step [1201/6243], Loss: 2.5777\n",
      "Epoch [9/10], Step [1301/6243], Loss: 2.6992\n",
      "Epoch [9/10], Step [1401/6243], Loss: 2.7998\n",
      "Epoch [9/10], Step [1501/6243], Loss: 2.2473\n",
      "Epoch [9/10], Step [1601/6243], Loss: 2.2943\n",
      "Epoch [9/10], Step [1701/6243], Loss: 2.7638\n",
      "Epoch [9/10], Step [1801/6243], Loss: 1.9463\n",
      "Epoch [9/10], Step [1901/6243], Loss: 2.5616\n",
      "Epoch [9/10], Step [2001/6243], Loss: 2.6253\n",
      "Epoch [9/10], Step [2101/6243], Loss: 2.5424\n",
      "Epoch [9/10], Step [2201/6243], Loss: 2.5441\n",
      "Epoch [9/10], Step [2301/6243], Loss: 2.8860\n",
      "Epoch [9/10], Step [2401/6243], Loss: 1.9933\n",
      "Epoch [9/10], Step [2501/6243], Loss: 2.4836\n",
      "Epoch [9/10], Step [2601/6243], Loss: 2.4626\n",
      "Epoch [9/10], Step [2701/6243], Loss: 2.5971\n",
      "Epoch [9/10], Step [2801/6243], Loss: 2.4573\n",
      "Epoch [9/10], Step [2901/6243], Loss: 3.0932\n",
      "Epoch [9/10], Step [3001/6243], Loss: 2.4594\n",
      "Epoch [9/10], Step [3101/6243], Loss: 2.9214\n",
      "Epoch [9/10], Step [3201/6243], Loss: 2.2288\n",
      "Epoch [9/10], Step [3301/6243], Loss: 2.9898\n",
      "Epoch [9/10], Step [3401/6243], Loss: 2.1821\n",
      "Epoch [9/10], Step [3501/6243], Loss: 2.7670\n",
      "Epoch [9/10], Step [3601/6243], Loss: 2.2810\n",
      "Epoch [9/10], Step [3701/6243], Loss: 2.6197\n",
      "Epoch [9/10], Step [3801/6243], Loss: 2.3583\n",
      "Epoch [9/10], Step [3901/6243], Loss: 2.1415\n",
      "Epoch [9/10], Step [4001/6243], Loss: 2.0556\n",
      "Epoch [9/10], Step [4101/6243], Loss: 2.3325\n",
      "Epoch [9/10], Step [4201/6243], Loss: 2.6090\n",
      "Epoch [9/10], Step [4301/6243], Loss: 2.7941\n",
      "Epoch [9/10], Step [4401/6243], Loss: 2.3371\n",
      "Epoch [9/10], Step [4501/6243], Loss: 2.3730\n",
      "Epoch [9/10], Step [4601/6243], Loss: 2.4310\n",
      "Epoch [9/10], Step [4701/6243], Loss: 2.2279\n",
      "Epoch [9/10], Step [4801/6243], Loss: 2.2813\n",
      "Epoch [9/10], Step [4901/6243], Loss: 2.6236\n",
      "Epoch [9/10], Step [5001/6243], Loss: 2.3632\n",
      "Epoch [9/10], Step [5101/6243], Loss: 2.5730\n",
      "Epoch [9/10], Step [5201/6243], Loss: 2.4726\n",
      "Epoch [9/10], Step [5301/6243], Loss: 2.3362\n",
      "Epoch [9/10], Step [5401/6243], Loss: 2.7750\n",
      "Epoch [9/10], Step [5501/6243], Loss: 2.4480\n",
      "Epoch [9/10], Step [5601/6243], Loss: 2.4963\n",
      "Epoch [9/10], Step [5701/6243], Loss: 2.1983\n",
      "Epoch [9/10], Step [5801/6243], Loss: 2.4488\n",
      "Epoch [9/10], Step [5901/6243], Loss: 2.1312\n",
      "Epoch [9/10], Step [6001/6243], Loss: 2.4835\n",
      "Epoch [9/10], Step [6101/6243], Loss: 2.4696\n",
      "Epoch [9/10], Step [6201/6243], Loss: 2.3108\n",
      "Epoch [10/10], Step [1/6243], Loss: 2.5702\n",
      "Epoch [10/10], Step [101/6243], Loss: 3.0267\n",
      "Epoch [10/10], Step [201/6243], Loss: 2.3605\n",
      "Epoch [10/10], Step [301/6243], Loss: 2.5117\n",
      "Epoch [10/10], Step [401/6243], Loss: 2.9445\n",
      "Epoch [10/10], Step [501/6243], Loss: 2.4049\n",
      "Epoch [10/10], Step [601/6243], Loss: 3.1505\n",
      "Epoch [10/10], Step [701/6243], Loss: 2.3623\n",
      "Epoch [10/10], Step [801/6243], Loss: 2.0799\n",
      "Epoch [10/10], Step [901/6243], Loss: 2.4589\n",
      "Epoch [10/10], Step [1001/6243], Loss: 2.5895\n",
      "Epoch [10/10], Step [1101/6243], Loss: 2.8257\n",
      "Epoch [10/10], Step [1201/6243], Loss: 2.6814\n",
      "Epoch [10/10], Step [1301/6243], Loss: 2.6753\n",
      "Epoch [10/10], Step [1401/6243], Loss: 2.6190\n",
      "Epoch [10/10], Step [1501/6243], Loss: 2.3204\n",
      "Epoch [10/10], Step [1601/6243], Loss: 2.2906\n",
      "Epoch [10/10], Step [1701/6243], Loss: 2.1003\n",
      "Epoch [10/10], Step [1801/6243], Loss: 2.1073\n",
      "Epoch [10/10], Step [1901/6243], Loss: 2.4442\n",
      "Epoch [10/10], Step [2001/6243], Loss: 2.8874\n",
      "Epoch [10/10], Step [2101/6243], Loss: 2.2318\n",
      "Epoch [10/10], Step [2201/6243], Loss: 2.3542\n",
      "Epoch [10/10], Step [2301/6243], Loss: 2.1391\n",
      "Epoch [10/10], Step [2401/6243], Loss: 2.4501\n",
      "Epoch [10/10], Step [2501/6243], Loss: 2.5406\n",
      "Epoch [10/10], Step [2601/6243], Loss: 2.2311\n",
      "Epoch [10/10], Step [2701/6243], Loss: 2.1925\n",
      "Epoch [10/10], Step [2801/6243], Loss: 2.5860\n",
      "Epoch [10/10], Step [2901/6243], Loss: 2.4570\n",
      "Epoch [10/10], Step [3001/6243], Loss: 3.2082\n",
      "Epoch [10/10], Step [3101/6243], Loss: 2.3799\n",
      "Epoch [10/10], Step [3201/6243], Loss: 2.6909\n",
      "Epoch [10/10], Step [3301/6243], Loss: 2.3214\n",
      "Epoch [10/10], Step [3401/6243], Loss: 2.2346\n",
      "Epoch [10/10], Step [3501/6243], Loss: 2.6181\n",
      "Epoch [10/10], Step [3601/6243], Loss: 2.6250\n",
      "Epoch [10/10], Step [3701/6243], Loss: 2.2145\n",
      "Epoch [10/10], Step [3801/6243], Loss: 2.1553\n",
      "Epoch [10/10], Step [3901/6243], Loss: 2.8539\n",
      "Epoch [10/10], Step [4001/6243], Loss: 2.3944\n",
      "Epoch [10/10], Step [4101/6243], Loss: 2.6626\n",
      "Epoch [10/10], Step [4201/6243], Loss: 2.5935\n",
      "Epoch [10/10], Step [4301/6243], Loss: 2.4781\n",
      "Epoch [10/10], Step [4401/6243], Loss: 2.2258\n",
      "Epoch [10/10], Step [4501/6243], Loss: 2.0490\n",
      "Epoch [10/10], Step [4601/6243], Loss: 2.3199\n",
      "Epoch [10/10], Step [4701/6243], Loss: 2.5780\n",
      "Epoch [10/10], Step [4801/6243], Loss: 2.5879\n",
      "Epoch [10/10], Step [4901/6243], Loss: 2.2299\n",
      "Epoch [10/10], Step [5001/6243], Loss: 2.6229\n",
      "Epoch [10/10], Step [5101/6243], Loss: 2.5110\n",
      "Epoch [10/10], Step [5201/6243], Loss: 2.3176\n",
      "Epoch [10/10], Step [5301/6243], Loss: 2.1578\n",
      "Epoch [10/10], Step [5401/6243], Loss: 2.1632\n",
      "Epoch [10/10], Step [5501/6243], Loss: 2.2551\n",
      "Epoch [10/10], Step [5601/6243], Loss: 2.2320\n",
      "Epoch [10/10], Step [5701/6243], Loss: 2.5553\n",
      "Epoch [10/10], Step [5801/6243], Loss: 2.3811\n",
      "Epoch [10/10], Step [5901/6243], Loss: 2.8270\n",
      "Epoch [10/10], Step [6001/6243], Loss: 2.5376\n",
      "Epoch [10/10], Step [6101/6243], Loss: 2.1945\n",
      "Epoch [10/10], Step [6201/6243], Loss: 2.6361\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, word, word_to_idx, vocab_size):\n",
    "    \"\"\"Predicts the next word based on the given word.\"\"\"\n",
    "    with torch.no_grad():  # Disable gradient calculation for prediction\n",
    "        bow = word_to_bow(word, vocab_size)\n",
    "        bow = bow.unsqueeze(0)  # Add a batch dimension for the model\n",
    "        output = model(bow)\n",
    "        _, predicted_idx = torch.max(output.data, dim=1)\n",
    "        predicted_word = [word for word, idx in word_to_idx.items() if idx == predicted_idx.item()][0]\n",
    "        return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next word for 'programación': J\n"
     ]
    }
   ],
   "source": [
    "sample_word = \"programación\"\n",
    "predicted_word = predict_next_word(model, sample_word, word_to_idx, vocab_size)\n",
    "print(f\"Predicted next word for '{sample_word}': {predicted_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
